---
layout: post
title:  "A data set for roulette prediction"
date:   2016-09-09 14:49:02 -0700
categories: jekyll update
---

## Disclaimer

This is not legal advice. Don't get your ass beat by a pit boss. Et cetera. 

## History

Roulette is unique among casino games in that it is, at some level, deterministic. When the ball is set in motion around the wheel the result of the game is already determined, though it may be difficult to predict. The croupier continues to accept bets for an appreciable amount of time after the game starts, which could allow the player to collect enough information about ball's speed and acceleration to predict where it will ultimately land.  

Because of this, the game of roulette has attracted attention from engineers for decades. It is widely thought that the first wearable computer was built by Edward Thorpe and Claude Shannon in Boston between 1960 and 1961 [1]. Using a miniature analog computer small enough to be hidden in a shoe, Thorpe and Shannon were able to predict which octant of the roulette wheel the ball would land. They estimated that using their predition technique they could gain 44% advantage over the casino. Since then, the use of predictive devices has been outlawed in a number of jurisdictions including Nevada and New Jersey [2], though there are reports of the police allowing million dollar winnings to be kept by players using predictive devices elsewhere in the world [3]. 

## Context

I became interested in computer roulette prediction during a [course on embedded systems at Berkeley](https://chess.eecs.berkeley.edu/eecs149/), where I was part of a project that attempted to build a digital shoe computer to replicate Thorpe and Shannon's work. I hope to write more about that system in a future post, but I'm writing this post now because I'm currently helping to teach a [freshman level electrical engineering course](http://inst.eecs.berkeley.edu/~ee16b/fa16/). This course introduces the discrete Fourier transform (DFT) as a basic introduction to "frequency-domain" thinking. I thought that it would be great to allow students to see how the DFT could be applied to better understand an interesting real-world dataset, and roulette prediction is a perfect example. 

## Data collection 

I was lucky enough to be able to borrow a miniature roulette wheel from Berkeley's computer graphics group. They had been using the wheel to illustrate algorithms for [selectively de-animating video footage](http://graphics.berkeley.edu/papers/Bai-SDV-2012-08/). My project partner Jon Chen and I experimented with a number of methods of acquiring reliable data about the dynamics of the roulette ball. 





[1] E.O. Thorp. 1998. The Invention of the First Wearable Computer. In Proceedings of the 2nd IEEE International Symposium on Wearable Computers (ISWC ’98). IEEE Computer Society, Washington, DC, USA, 4-8.        

[2] D.W. Schnell-Davis. 2012. High-Tech Casino Advantage Play: Legislative Approaches to the Threat of Predictive Devices. UNLV Gaming Law Journal 3:2. Art. 7.

[3] 'Laser scam' gamblers to keep £1m, BBC News. http://news.bbc.co.uk/2/hi/uk_news/4069629.stm 


Interpolation is a way to estimate a function \\( f(x) \\) from samples. We assume that we are given a set of points \\( (x_1, x_2, \dots ,x_n) \\) known as knots along with the value of the function at those knots 
\\[ (y_1, y_2, \dots, y_n) = (f(x_1), f(x_2), \dots, f(x_n)). \\] 


The fundamental building blocks of an interpolation method are the basis functions: a set of functions \\(\varphi_k(x)\\) that take the value 1 at a particular knot \\(x_k\\) and the value 0 at every other knot. Once we have basis functions we can interpolate between knots simply by taking the linear combination 
\\[
\hat f(x) = \sum_{k=1}^n y_k \varphi_k(x). 
\\]
Different methods of constructing basis functions with this property lead to different interpolation methods. To illustrate some of the choices and trade-offs between interpolation methods, we consider the problem of reconstructing the functions \\( f(x) = x^3 - 25x\\)  and \\( g(x) = x - 5 \lfloor \frac{1}{5} x \rfloor \\) from uniformly-spaced samples. 

<img width="350" src="/figs/sampled_function_cubic.svg"> 
<img width="350" src="/figs/sampled_function_sawtooth.svg"> 









## Shiftable basis functions 

We first look at three methods that use shiftable basis functions. That is, there is a single function \\( \phi(x) \\) such that all the basis functions can be generated by shifting \\( \phi \\) to \\(x_k\\): 
\\[
 \varphi_k(x) - \phi(x - x_k). 
\\]




# Nearest-neighbor interpolation

This is probably the simplest method of interpolation. To estimate the value of the function at a point \\(x\\), we just use the value of the function at the knot \\(x_k\\) that is nearest to \\(x\\). This leads to rectangular basis functions \\(\varphi_k(x)\\) that take the value 1 at all point that are nearer to \\(x_k\\) than any other knot, and take the value 0 elsewhere. These are shiftable basis functions that correspond to the rect filter 
\\[
\phi(x) = \begin{cases} 1 & \textrm{if } |x| \le 0.5 \\\ 0 & \textrm{otherwise.} \end{cases}
\\]

The basis function is shown here:

<img style="display: block; margin: 0 auto;" width="350" src="/figs/nearest_neighbors_basis_function.svg"> 

The result of nearest neighbor interpolation is shown here: 

<img width="350" src="/figs/nearest_neighbors_interpolation_cubic.svg"> 
<img width="350" src="/figs/nearest_neighbors_interpolation_sawtooth.svg"> 




# Linear interpolation

Linear interpolation connects each pair of neighboring knots \\( (x_k, y_k) \\) and \\( (x_{k+1}, y_{k+1}) \\)with a line segment. This results in interpolation by a piecewise affine function. The shiftable basis function used for linear interpolation is shown here:

<img style="display: block; margin: 0 auto;" width="350" src="/figs/linear_interpolation_basis_function.svg"> 


The result of linear interpolation is shown here: 

<img width="350" src="/figs/linear_interpolation_cubic.svg"> 
<img width="350" src="/figs/linear_interpolation_sawtooth.svg"> 




# Sinc interpolation 

Sinc interpolation results from interpolation using shifted sinc basis functions
\\[
  \phi(x) = \operatorname{sinc}(x) = \frac{\sin(\pi x)}{\pi x}. 
\\]
This basis function is shown here: 

<img style="display: block; margin: 0 auto;" width="350" src="/figs/sinc_interpolation_basis_function.svg"> 

Sinc interpolation is important in the fields of harmonic analysis and signal processing where it is often used to reconstruct band-limited signals from samples. The result of sinc interpolation is shown here:

<img width="350" src="/figs/sinc_interpolation_cubic.svg"> 
<img width="350" src="/figs/sinc_interpolation_sawtooth.svg"> 









## Non-shiftable basis functions

We now look at some additional interpolation methods that use basis functions whose shape changes as a function of \\(k\\). Here we will plot both the basis function corresponding to \\(x_k = 0\\), but also all the basis functions on the same axes. 




# Polynomial interpolation 

Polynomial interpolation, sometimes known as Lagrange interpolation, involves fitting a polynomial to the data points. To fit \\(n\\) points requires a polynomial of degree \\(n-1\\). Each of the basis functions are themselves polynomials of degree \\(n-1\\) and can be computed using the formula 
\\[
\varphi_k(x) = 
\prod_{\begin{smallmatrix}1 \leq m\leq n\\\ m \neq k\end{smallmatrix}}{\frac {x-x_{m}}{x_{k}-x_{m}}}.
\\]

These basis functions are illustrated here:

<img width="350" src="/figs/polynomial_interpolation_basis_function.svg"> 
<img width="350" src="/figs/polynomial_interpolation_basis_functions.svg"> 

The result of nearest polynomial interpolation is shown here: 

<img width="350" src="/figs/polynomial_interpolation_cubic.svg"> 
<img width="350" src="/figs/polynomial_interpolation_sawtooth.svg"> 

We see that the polynomial basis functions can easily interpolate the cubic function (since it's a polynomial itself, the interpolation is exact!) but have a lot of trouble interpolating the sawtooth function. The existence of these artifacts in the sawtooth interpolation is known as Runge's phenomenon, which is a common problem in polynomial interpolation. 




# Kriging   

Kriging, or Gaussian process regression, is a method that performs an optimal interpolation based on a statistical prior over possible functions. It is often assumed that the values \\( y = f(x) \\) are produced from a Gaussian process \\( Y(x) \\) with mean 0 and known covariance \\( Cov(Y(x), Y(z)) = c(x, z) \\). The optimal basis functions can be computed in terms of the covariance \\(c\\) using the formula
\\[
\begin{bmatrix}
\varphi_1(x) \\\ \vdots \\\ \varphi_n(x)
\end{bmatrix}  
= \begin{bmatrix}
c(x_1,x_1) & \cdots & c(x_1,x_n) \\\
\vdots & \ddots & \vdots  \\\
c(x_n,x_1) & \cdots & c(x_n,x_n) \end{bmatrix}^{-1}
\begin{bmatrix}
c(x_1,x) \\\ \vdots \\\ c(x_n,x) 
\end{bmatrix}
\\]
The basis functions corresponding to the squared exponential covariance function
\\[
c(x, z) = e^{-\alpha \|x - z\|^2} 
\\]
for \\( \alpha = 1/2 \\) are illustrated here: 

<img width="350" src="/figs/kriging_basis_function.svg"> 
<img width="350" src="/figs/kriging_basis_functions.svg"> 

The result of interpolation by Kriging is shown here:

<img width="350" src="/figs/kriging_cubic.svg"> 
<img width="350" src="/figs/kriging_sawtooth.svg"> 




# Cubic splines

Cubic spline interpolation is another method that relies on piecewise cubic interpolation between knots. The splines are constrained to be continuous and to have continuous first and second derivatives, which leads to a tridiagonal system of linear equations to solve for the cubic polynomials. The basis functions corresponding to cubic spline interpolation are illustrated here: 

<img width="350" src="/figs/cubic_spline_basis_function.svg"> 
<img width="350" src="/figs/cubic_spline_basis_functions.svg"> 

The result of cubic spline interpolation is shown here: 


<img width="350" src="/figs/cubic_spline_interpolation_cubic.svg"> 
<img width="350" src="/figs/cubic_spline_interpolation_sawtooth.svg"> 

